{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Homework \u2014 Empirical Interpolation Method (EIM)\n", "\n", "**Context.** You will implement the Empirical Interpolation Method (EIM) to recover an affine expansion of a non\u2011affine source term depending on three parameters. The goal is to build the greedy EIM basis and interpolation points (\"magic points\"), test the online reconstruction from only \\(M\\) samples, and study stability and error indicators.\n", "\n", "**References to the course slides (page numbers):**\n", "- EIM idea & interpolant: pp. 8\u201311.\n", "- Greedy algorithm (pseudocode): p. 16 (*EIM algorithm i*).\n", "- Properties (lower triangular, unit diagonal): pp. 18\u201321.\n", "- Error analysis with Lebesgue constant: pp. 23\u201326.\n", "- One\u2011point a posteriori estimator: pp. 28\u201331.\n", "- Practical discretization (algebraic form): pp. 34\u201338.\n", "- DEIM overview (optional extension): pp. 40\u201347.\n", "\n", "You only need standard Python + NumPy + Matplotlib. Avoid external packages.\n", "\n", "**Submission.** When you finish, ensure all `TODO` markers are addressed and the notebook runs top\u2011to\u2011bottom producing: (i) the learned EIM basis/points, (ii) an error\u2011vs\u2011M plot on a test set, (iii) a plot of the (discrete) Lebesgue constant, and (iv) a plot showing the effectivity of the one\u2011point estimator.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 0. Imports & helpers\n", "Do **not** use seaborn or style packages; use plain Matplotlib."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "from dataclasses import dataclass\n", "\n", "np.random.seed(42)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Problem setup\n", "We work on \\(\\Omega=[0,1]\\). The source depends on three parameters \\(\\mu=(\\mu_1,\\mu_2,\\mu_3)\\) and is given by\n", "\\[ s(x;\\mu) = (1+\\mu_1)\\,\\sin\\big((1+\\mu_2)\\pi x\\big) + \\exp\\big(-20(x-\\mu_3)^2\\big). \\]\n", "You will approximate \\(s(\\cdot;\\mu)\\) using EIM so that linear forms become affine in \\(\\mu\\) (see slides pp. 3\u20136)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Discretize the domain\n", "Nq = 1000\n", "x = np.linspace(0.0, 1.0, Nq)\n", "\n", "# Parameter ranges: mu1 in [0,2], mu2 in [0,2], mu3 in [0.2,0.8]\n", "mu_box = np.array([[0.0, 2.0],\n", "                   [0.0, 2.0],\n", "                   [0.2, 0.8]])\n", "\n", "def s_fun(x, mu):\n", "    mu1, mu2, mu3 = mu\n", "    return (1+mu1)*np.sin((1+mu2)*np.pi*x) + np.exp(-20.0*(x-mu3)**2)\n", "\n", "def sample_mu(n):\n", "    u = np.random.rand(n, 3)\n", "    return mu_box[:,0] + u*(mu_box[:,1]-mu_box[:,0])\n", "\n", "# Build training/test parameter sets (adjust sizes if needed)\n", "n_train = 400\n", "n_test  = 200\n", "mu_train = sample_mu(n_train)\n", "mu_test  = sample_mu(n_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Quick visualization (sanity check)\n", "Plot a few random sources to get a feel for the function family."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for _ in range(3):\n", "    mu0 = sample_mu(1)[0]\n", "    plt.figure(figsize=(6,3))\n", "    plt.plot(x, s_fun(x, mu0))\n", "    plt.xlabel('x'); plt.ylabel('s(x;mu)'); plt.title(f'sample mu={mu0}')\n", "    plt.grid(True)\n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Building blocks\n", "### 2.1 Lower\u2011triangular solve (forward substitution)\n", "**Task.** Implement a function that solves \\(L\\,\\gamma = b\\) where \\(L\\) is lower\u2011triangular with **unit diagonal**.\n", "\n", "Hint: This will be used with the interpolation matrix \\(B_M = Q_I\\) (slides p. 37), which is lower triangular with ones on the diagonal (slides pp. 18\u201321)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def forward_solve(L, b):\n", "    \"\"\"\n", "    Solve L x = b for lower-triangular L with ones on the diagonal.\n", "    If b is 2D, solve column-wise.\n", "    TODO: implement forward substitution.\n", "    \"\"\"\n", "    # --- YOUR CODE HERE ---\n", "    raise NotImplementedError('forward_solve')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.2 Data container\n", "We will store the learned basis, point indices, and diagnostics in a simple dataclass."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["@dataclass\n", "class EIMModel:\n", "    Q: np.ndarray        # (Nq, M)\n", "    I: list              # magic point indices (length M)\n", "    mu_eim: list         # selected parameters (length M)\n", "    t_history: list      # evolution of magic points (indices) per step\n", "    err_train: list      # training sup-norm errors per M\n", "    lebesgue: list       # discrete Lebesgue constants per M (optional)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. EIM offline (greedy)\n", "**Task.** Implement the greedy EIM algorithm to build \\(Q=[\\rho_1|\\dots|\\rho_M]\\) and the set of indices \\(I=\\{i_1,\\dots,i_M\\}\\).\n", "\n", "Use the practical, *discrete* version on the grid \\(\\Omega_h=\\{x_k\\}_{k=1}^{N_q}\\) and on the training set \\(\\Xi_{\\text{train}}\\). See slides pp. 34\u201338 for the algebraic form and p. 16 for the greedy pseudocode.\n", "\n", "Minimal checklist:\n", "- Start with \\(\\mu_1 = \\arg\\max_\\mu \\|g(\\cdot;\\mu)\\|_\\infty\\), set \\(\\xi_1=g(\\cdot;\\mu_1)\\). Pick \\(t_1=\\arg\\max_x |\\xi_1(x)|\\), define \\(\\rho_1=\\xi_1/\\xi_1(t_1)\\).\n", "- At step \\(m+1\\): pick \\(\\mu_{m+1}\\) maximizing the current error, set residual \\(r_{m+1}=\\xi_{m+1}-I_m^x\\xi_{m+1}\\), choose \\(t_{m+1}=\\arg\\max |r_{m+1}|\\), and \\(\\rho_{m+1}=r_{m+1}/r_{m+1}(t_{m+1})\\).\n", "- Maintain the interpolation system \\(B_m\\,\\gamma=g_I\\) with \\(B_m=Q_m[I_m,:]\\)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def eim_offline(s_fun, x, mu_train, M_max=20, tol=1e-6):\n", "    \"\"\"Greedy EIM on a discrete grid.\n", "    Returns an EIMModel with fields filled.\n", "    TODO: implement the algorithm described on the slides.\n", "    Hints:\n", "      * Precompute training snapshots S[:,j] = s_fun(x, mu_train[j]).\n", "      * Define a helper that, given Q_m and I_m, reconstructs any snapshot by solving Q_m[I_m,:] gamma = snapshot[I_m].\n", "      * Track the max training sup-norm error after each enrichment.\n", "      * Optionally compute the discrete Lebesgue constant: max_x sum_i |(Q_m B_m^{-1})[x,i]|.\n", "    \"\"\"\n", "    # --- YOUR CODE HERE ---\n", "    raise NotImplementedError('eim_offline')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. EIM online\n", "**Task.** Given \\(Q\\) and \\(I\\), reconstruct \\(s_M(\\cdot;\\mu) = Q\\,\\gamma(\\mu)\\) from just the \\(M\\) samples \\(s(x_{I_j};\\mu)\\).\n", "\n", "Slides pp. 36\u201338 show the algebraic online step."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def eim_online(model, x, mu):\n", "    \"\"\"Return (approx, gamma) with approx in R^{Nq}. Uses only M samples.\n", "    TODO: implement using forward_solve on Q[I,:] and g_I.\n", "    \"\"\"\n", "    # --- YOUR CODE HERE ---\n", "    raise NotImplementedError('eim_online')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Run the offline phase and sanity checks\n", "After you implement `eim_offline`, run it and test the two key properties (slides pp. 18\u201321):\n", "- `Q[I,:]` is lower triangular with unit diagonal.\n", "- Interpolation property on the training set: for each \\(\\mu\\), the interpolant matches the function at the magic points."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Uncomment after implementing eim_offline\n", "# model = eim_offline(s_fun, x, mu_train, M_max=25, tol=1e-8)\n", "# QI = model.Q[model.I,:]\n", "# # Triangular + ones on diagonal (numerically)\n", "# assert np.allclose(np.diag(QI), 1.0)\n", "# assert np.allclose(QI, np.tril(QI))\n", "# # Interpolation holds on training samples at magic points\n", "# S_train = np.stack([s_fun(x, mu) for mu in mu_train], axis=1)\n", "# B = model.Q[model.I,:]\n", "# gamma = forward_solve(B, S_train[model.I,:])\n", "# interp = model.Q @ gamma\n", "# assert np.allclose(interp[model.I,:], S_train[model.I,:])\n", "# print('Sanity checks passed.')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6. Error on a test set\n", "**Task.** Compute the test error curve \\(\\max_{x,\\,\\mu\\,\\in\\,\\text{test}}|s - I_M s|\\) for \\(M=1,\\dots, M_\\text{max}\\)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def test_error_curve(model, x, mu_test):\n", "    \"\"\"Return an array errs[M-1] with sup error over x,mu for each M.\n", "    TODO: for each m=1..Mmax, solve on the m-by-m system and reconstruct.\n", "    \"\"\"\n", "    # --- YOUR CODE HERE ---\n", "    raise NotImplementedError('test_error_curve')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Plot: error decay vs. M\n", "Produce a semilog plot of the error vs. \\(M\\). If your training set is representative, you should see rapid decay (cf. the convergence remarks in the slides\u2019 error analysis)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# After implementing test_error_curve and building `model`:\n", "# errs = test_error_curve(model, x, mu_test)\n", "# plt.figure(figsize=(6,4))\n", "# plt.semilogy(np.arange(1, len(errs)+1), errs)\n", "# plt.xlabel('M (basis size)')\n", "# plt.ylabel('max_{x, mu in test} |s - I_M s|')\n", "# plt.title('EIM error decay on test set')\n", "# plt.grid(True)\n", "# plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7. Discrete Lebesgue constant\n", "Recall the discrete analogue \\(\\Lambda_M = \\max_x \\sum_i |\\ell^M_i(x)|\\) where \\(\\ell^M_i\\) are the Lagrange functions (slides pp. 23\u201326). With matrices, this can be computed as `max(sum(abs(Q @ inv(B)), axis=1))` for each \\(M\\), where `B = Q[I,:]`.\n", "\n", "**Task.** Implement a function that returns the sequence \\(\\{\\Lambda_m\\}_{m=1}^M\\)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def discrete_lebesgue_constants(Q, I):\n", "    \"\"\"Return array [Lambda_1, ..., Lambda_M].\n", "    TODO: loop over m, compute inv(Qm[Im,:]) and the max row-sum norm of Qm @ invB.\n", "    \"\"\"\n", "    # --- YOUR CODE HERE ---\n", "    raise NotImplementedError('discrete_lebesgue_constants')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Plot: Lebesgue constant growth\n", "Plot \\(\\Lambda_M\\) vs. \\(M\\). Compare its growth with what is discussed on the slides (the bound \\(\\Lambda_M \\le 2^{M-1}\\) is pessimistic)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Lambda_seq = discrete_lebesgue_constants(model.Q, model.I)\n", "# plt.figure(figsize=(6,4))\n", "# plt.plot(np.arange(1, len(Lambda_seq)+1), Lambda_seq)\n", "# plt.xlabel('M')\n", "# plt.ylabel('Lebesgue constant (discrete)')\n", "# plt.title('Lebesgue constant growth')\n", "# plt.grid(True)\n", "# plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 8. One\u2011point estimator\n", "Let \\(t_{M+1}\\) be the next magic point (recorded during offline). Define \\(\\Delta_M(\\mu)=|s(t_{M+1};\\mu)-I^x_M s(t_{M+1};\\mu)|\\). It is a lower bound of the true sup error and equals it if \\(s(\\cdot;\\mu)\\in X_{M+1}\\) (slides pp. 28\u201331).\n", "\n", "**Task.** Implement a function that computes the **median** effectivity \\(\\operatorname{median}_\\mu \\Delta_M(\\mu)/\\|s-I^x_M s\\|_\\infty\\) on a test set for \\(M=1,\\dots,M_{\\max}-1\\)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def one_point_effectivity(model, x, mu_test):\n", "    \"\"\"Return effectivity ratios (median over test mu) for m=1..M-1.\n", "    TODO: for each m, reconstruct with Q[:,:m], I[:m];\n", "          compute true sup error over x and the one-point delta at t_{m+1}.\n", "    \"\"\"\n", "    # --- YOUR CODE HERE ---\n", "    raise NotImplementedError('one_point_effectivity')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Plot: one\u2011point estimator effectivity\n", "Plot the median effectivity vs. \\(M\\)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# eff = one_point_effectivity(model, x, mu_test)\n", "# plt.figure(figsize=(6,4))\n", "# plt.plot(np.arange(1, len(eff)+1), eff)\n", "# plt.xlabel('M')\n", "# plt.ylabel('median(\u0394_M / true error)')\n", "# plt.title('One\u2011point estimator effectivity (median)')\n", "# plt.grid(True)\n", "# plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 9. Using EIM inside a linear form (short demo)\n", "Consider a test function \\(v(x)\\) and approximate \\(\\int_0^1 s(x;\\mu)v(x)\\,dx\\) with the trapezoidal rule on \\(\\Omega_h\\). With EIM, precompute \\(\\int_0^1 \\rho_j(x) v(x)\\,dx\\) once, then online evaluate \\(\\sum_j \\gamma_j(\\mu)\\,\\int \\rho_j v\\). See slides pp. 5\u20136 and 50\u201351.\n", "\n", "**Task.** Implement the offline precompute and online evaluation for two simple test functions (e.g. `sin(pi x)` and `x(1-x)`). Report absolute errors."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def trapezoid_weights(x):\n", "    w = np.zeros_like(x)\n", "    w[1:-1] = (x[2:] - x[:-2]) / 2.0\n", "    w[0] = (x[1]-x[0]) / 2.0\n", "    w[-1] = (x[-1]-x[-2]) / 2.0\n", "    return w\n", "\n", "# --- YOUR CODE HERE (after model is built) ---\n", "# w = trapezoid_weights(x)\n", "# v1 = np.sin(np.pi*x)\n", "# v2 = x*(1-x)\n", "# coeff_v1 = ...    # precompute integrals of basis against v1\n", "# coeff_v2 = ...\n", "# mu0 = sample_mu(1)[0]\n", "# # truth\n", "# truth_v1 = float(np.sum(w * s_fun(x, mu0) * v1))\n", "# truth_v2 = float(np.sum(w * s_fun(x, mu0) * v2))\n", "# # EIM online\n", "# approx_v1 = ...  # dot(coeff_v1, gamma(mu0))\n", "# approx_v2 = ...\n", "# print('abs error v1:', abs(truth_v1-approx_v1))\n", "# print('abs error v2:', abs(truth_v2-approx_v2))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 10. (Optional) DEIM variant\n", "Reimplement the approximation using a **DEIM** basis (POD of snapshot matrix) while keeping the same greedy selection of indices (slides pp. 40\u201347). Compare the error vs. the first discarded singular value \\(\\sigma_{M+1}\\).\n", "\n", "Checklist:\n", "- Build a snapshot matrix `S` (e.g. using the training set).\n", "- Compute its SVD and form `Q = U[:, :M]`.\n", "- Select indices greedily (same loop as in EIM but on basis columns).\n", "- Online: solve `Q[I,:] gamma = g_I` and reconstruct.\n", "- Compare `||g - g_M||_2` on a test set to `sigma_{M+1}`.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# --- YOUR CODE HERE (optional) ---\n", "pass"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 11. Short questions (answer briefly in markdown)\n", "1. Why is `Q[I,:]` lower triangular with ones on the diagonal? (slides pp. 18\u201321)\n", "2. State and interpret the Lebesgue\u2011constant error bound (slides pp. 23\u201326). When can the bound be pessimistic?\n", "3. Under which condition does the one\u2011point estimator equal the true \\(L^\\infty\\) error? (slides pp. 28\u201331)\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}